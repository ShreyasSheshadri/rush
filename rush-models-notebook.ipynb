{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython\n",
    "from IPython.display import clear_output\n",
    "#import pandas as pd\n",
    "import imp\n",
    "imp.load_module('pandas', f, pathname, desc)\n",
    "import sqlite3\n",
    "import sys\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import scipy\n",
    "import pdb\n",
    "import cython\n",
    "from ipyparallel import Client\n",
    "import copy\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes, inset_axes, mark_inset\n",
    "import sklearn.neighbors\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import cPickle as pickle\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (18,3)\n",
    "matplotlib.rcParams['axes.linewidth'] = 2\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "matplotlib.rc('xtick', labelsize=16)\n",
    "matplotlib.rc('ytick', labelsize=16)\n",
    "\n",
    "SECONDS_1960_1970 = 315619200 # SAS epoch is Jan 1, 1960. UNIX is Jan 1, 1970.\n",
    "DAYS_1960_1970 = 315619200 / 86400.\n",
    "\n",
    "np.random.seed(42)\n",
    "RED = '#e41a1c'\n",
    "BLUE = '#377eb8'\n",
    "GREEN = '#4daf4a'\n",
    "PURPLE = '#984ea3'\n",
    "ORANGE = '#ff7f00'\n",
    "YELLOW = '#e6ab02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'hello world', 1: 'test'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client()\n",
    "v = c[:]\n",
    "print c.ids\n",
    "v.map(lambda x: x, ['hello world', 'test']).get_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pxlocal(line, cell):\n",
    "    ip = get_ipython()\n",
    "    ip.run_cell_magic('px', line, cell)\n",
    "    ip.run_cell(cell)\n",
    "ip = get_ipython()\n",
    "ip.register_magic_function(pxlocal, 'cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%pxlocal\n",
    "class SimplePoissonProcess:\n",
    "    _param = None\n",
    "    \n",
    "    def __init__(self, param=None):\n",
    "        self._param = param\n",
    "    \n",
    "    def _seq_negloglikelihood(self, param, data_epochs, observation_window):\n",
    "        nll = 0.0\n",
    "        for sequence in data_epochs:\n",
    "            nll += self._negloglikelihood(param, sequence, observation_window)\n",
    "        return nll/len(data_epochs)\n",
    "\n",
    "    def _negloglikelihood(self, param, data_epochs, observation_window):\n",
    "        # single sequence\n",
    "        start, end = observation_window\n",
    "        observation_window_length = end - start\n",
    "        total_num_transactions = len(data_epochs)\n",
    "        return -(total_num_transactions * np.log(param) - param * observation_window_length/1000.0)\n",
    "        \n",
    "    def fit(self, data_epochs, observation_window):\n",
    "        start, end = observation_window\n",
    "        observation_window_length = (end - start)/1000.0\n",
    "        total_num_transactions = np.sum([len(epochs) for epochs in data_epochs])\n",
    "        num_sequences = len(data_epochs)\n",
    "        return float(total_num_transactions)/(num_sequences * observation_window_length)\n",
    "    \n",
    "    def predict_dist(self, niter=1000):\n",
    "        simulated_iats = 1000.0 * np.random.exponential(scale=1.0/self._param, size=(niter))\n",
    "        return simulated_iats\n",
    "    \n",
    "    def intensity_integral(self, start_t, end_t,):\n",
    "        return self._param * (end_t - start_t) / 1000.0\n",
    "    \n",
    "    def intensity(self, t):\n",
    "        return self._param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%pxlocal\n",
    "class TimeVaryingPoissonProcess:\n",
    "    _lambda = None\n",
    "    _mu = None\n",
    "    _EPS = 10**-9 # numbers below this are set to 0\n",
    "\n",
    "    def __init__(self, lam=None, mu=None, observation_window=None):\n",
    "        self._lambda = lam\n",
    "        self._mu = mu\n",
    "        self._observation_window = observation_window\n",
    "    \n",
    "    def fit(self, F, D, data, observation_window, niter=1000):     \n",
    "        print 'Fitting...'\n",
    "        x0 = np.random.uniform(10**-4, 1.0, size=D.shape[0]+1)\n",
    "        bounds = [(self._EPS, None)] * (D.shape[0] + 1)\n",
    "            \n",
    "        x, f, d = scipy.optimize.fmin_l_bfgs_b(self._seq_negloglikelihood,\n",
    "                                               x0=x0,\n",
    "                                               args=(F, D, data, observation_window),\n",
    "                                               factr=10.0**3.0, iprint=99, pgtol=10**-12, approx_grad=False,\n",
    "                                               disp=99, bounds=bounds, callback=None, maxiter=niter, maxfun=niter)\n",
    "\n",
    "        return x, f, d\n",
    "    \n",
    "    def _seq_negloglikelihood(self, params, F, D, data, observation_window):\n",
    "        nsamples = len(data)\n",
    "        ll = []\n",
    "        gr = []\n",
    "        for i in range(nsamples):\n",
    "            ll_i, gr_i = self._negloglikelihood(params, F[i], D, data[i], observation_window)\n",
    "            ll.append(ll_i)\n",
    "            gr.append(gr_i)\n",
    "        return np.sum(ll)/nsamples, np.sum(np.array(gr), axis=0)/nsamples\n",
    "\n",
    "    @staticmethod\n",
    "    def _negloglikelihood(params, F, D, sequence, observation_window):\n",
    "        lam = params[0]\n",
    "        mu = params[1:]\n",
    "        T0, T = observation_window\n",
    "        T_length = (T - T0)/1000.0 # scale down\n",
    "        \n",
    "        loglikelihood = 0.0\n",
    "        grad_lambda = 0.0\n",
    "        grad_mu = [0.0] * mu.shape[0]\n",
    "\n",
    "        C = np.dot(mu, F) # C(i) = \\sum_j mu_j f_j(t_i)\n",
    "        Z = lam + C\n",
    "        loglikelihood = np.sum(np.log(Z)) - lam * T_length - np.sum(mu * D) # D is already in 1/1000 s\n",
    "        grad_lambda = np.sum(1.0/Z) - T_length\n",
    "        grad_mu = [np.sum(F[j]/Z) - D[j] for j in range(D.shape[0])]\n",
    "        gradient = np.concatenate(([grad_lambda], grad_mu), axis=0)\n",
    "        return -loglikelihood, -gradient\n",
    "\n",
    "    def check_gradient(self, F, D, data, observation_window, delta=10**-8):\n",
    "        print 'Checking gradient...'\n",
    "        for i in range(50):\n",
    "            row_idx = np.random.randint(0, len(data))\n",
    "            sequence = data[row_idx]\n",
    "            f = F[row_idx]\n",
    "            init_lambda = np.random.uniform(10**-6, 1.0)\n",
    "            init_mu = np.random.uniform(10**-6, 1.0, size=D.shape[0])\n",
    "            init_params = np.concatenate(([init_lambda], init_mu), axis=0)\n",
    "            \"\"\"\n",
    "            diff = scipy.optimize.check_grad(lambda p, a, f, d2, d, o, a2, b:\n",
    "                                                 self._negloglikelihood(p, a, f, d2, d, o, a2, b)[0],\n",
    "                                             lambda p, a, f, d2, d, o, a2, b:\n",
    "                                                 self._negloglikelihood(p, a, f, d2, d, o, a2, b)[1],\n",
    "                                             init_params,\n",
    "                                             alpha, f, D, sequence, observation_window, a2, b)\n",
    "            if diff > 10**-3:\n",
    "                print 'Warning: gradient checking failed, diff =', diff\n",
    "            \n",
    "            \"\"\"\n",
    "            #params = np.array([np.random.uniform(), np.random.uniform(), np.random.uniform()])\n",
    "            negloglikelihood, gradient = self._negloglikelihood(init_params, f, D,\n",
    "                                                                [sequence], observation_window)\n",
    "            \n",
    "            param_idx = np.random.randint(len(init_params))\n",
    "            #param_idx = len(init_params) - 1\n",
    "            perturbed_params = np.copy(init_params)\n",
    "            perturbed_params[param_idx] += delta\n",
    "            \n",
    "            #perturbed_A, perturbed_B, perturbed_W = self._compute_A_B([sequence], perturbed_params)\n",
    "            new_negloglikelihood, new_gradient = self._negloglikelihood(perturbed_params, f, D,\n",
    "                                                                        [sequence], observation_window)\n",
    "            \n",
    "            numerical_gradient = (new_negloglikelihood - negloglikelihood)/delta\n",
    "            #if abs((gradient[param_idx] - numerical_gradient)/numerical_gradient)  > 0.001:\n",
    "            if not np.isclose(gradient[param_idx], numerical_gradient):\n",
    "                print 'Gradient checking failed for parameter:', param_idx\n",
    "                print 'Analytical gradient:', gradient[param_idx],\n",
    "                print 'Numerical gradient:', (new_negloglikelihood - negloglikelihood)/delta\n",
    "    \n",
    "    def predict_dist(self, t0, niter=1000):\n",
    "        step = 3600.0 - (t0 % 3600)\n",
    "\n",
    "        simulated_iats = np.zeros(shape=(niter,))\n",
    "        for i in range(niter):\n",
    "            t = 0.0\n",
    "            while True:\n",
    "                upper_bound = self.intensity(t0 + t)\n",
    "                s = 1000.0 * np.random.exponential(1.0/upper_bound) # params are in 1/1000s units\n",
    "            \n",
    "                if s >= step: # upper bound no longer valid\n",
    "                    t += step\n",
    "                    step = 3600.0 # one hour in seconds\n",
    "                    continue\n",
    "                \n",
    "                t += s # move ahead in time (may accept or reject)\n",
    "                step -= s\n",
    "                u = np.random.uniform(0.0, 1.0)\n",
    "                lambda_t = self.intensity(t0 + t)\n",
    "                \n",
    "                if u <= lambda_t/upper_bound: # accept\n",
    "                    simulated_iats[i] = t\n",
    "                    break\n",
    "    \n",
    "        return simulated_iats\n",
    "\n",
    "    def intensity(self, t0):\n",
    "        t0_ = time.gmtime(t0)\n",
    "        hour_of_day, day_of_week, day_of_month = map(int, time.strftime(\"%H %w %d\", t0_).split(' '))\n",
    "        day_of_week = (day_of_week - 1) % 7\n",
    "        \n",
    "        f = np.array([int(hour_of_day == h) for h in range(24)] +\n",
    "                     [int(day_of_week >= 0 and day_of_week <= 3)] +\n",
    "                     [int(day_of_week == 4)] +\n",
    "                     [int(day_of_week >= 5 and day_of_week <= 6)] +\n",
    "                     [int(day_of_month == 1)], dtype=np.int)\n",
    "\n",
    "        return self._lambda + np.sum(self._mu * f)\n",
    "    \n",
    "    def intensity_integral(self, start_t, end_t):\n",
    "        integral_value = 0.0\n",
    "        \n",
    "        integral_value += self._lambda * (end_t - start_t) / 1000.0\n",
    "        \n",
    "        # mu term integral\n",
    "        d = np.zeros(28, dtype=np.float64)\n",
    "        start_timestamp = pd.to_datetime(start_t, unit='s')\n",
    "        end_timestamp = pd.to_datetime(end_t, unit='s')\n",
    "        \n",
    "        start_ts_roundup = start_timestamp.floor(freq='H')\n",
    "        start_seconds = (start_timestamp - start_ts_roundup).total_seconds()\n",
    "        start_hour = start_timestamp.hour\n",
    "        end_ts_roundwn = end_timestamp.ceil(freq='H')\n",
    "        end_seconds = (end_ts_roundwn - end_timestamp).total_seconds()\n",
    "        end_hour = end_timestamp.hour\n",
    "\n",
    "        #print 't_i-1', start_timestamp, 't_i', end_timestamp\n",
    "        \n",
    "        #hour_periods = pd.date_range(start_timestamp, end_timestamp, freq='s', closed='left')\n",
    "        hour_periods2 = pd.date_range(start_ts_roundup, end_ts_roundwn, freq='H', closed='left')\n",
    "        #print hour_periods\n",
    "        #hour_counts = Counter(hour_periods.hour)\n",
    "        hour_counts2 = Counter(hour_periods2.hour)\n",
    "        hour_counts2[start_hour] -= start_seconds/3600.0\n",
    "        hour_counts2[end_hour] -= end_seconds/3600.0\n",
    "        #print hour_counts2\n",
    "        #print start_timestamp, start_ts_roundup, end_timestamp, end_ts_roundwn\n",
    "        for h in range(24):\n",
    "            #d[h] = hour_counts[h] / 1000.0\n",
    "            d[h] = (hour_counts2[h] * 3600.0) / 1000.0\n",
    "            #print h, hour_counts[h], hour_counts2[h] * 3600.0\n",
    "            #assert abs(d[h] - (hour_counts2[h] * 3600.0) / 1000.0) < 10**-9\n",
    "        \n",
    "        start_ts_roundup = start_timestamp.floor(freq='D')\n",
    "        start_seconds = (start_timestamp - start_ts_roundup).total_seconds()\n",
    "        start_day = start_timestamp.weekday()\n",
    "        end_ts_roundwn = end_timestamp.ceil(freq='D')\n",
    "        end_seconds = (end_ts_roundwn - end_timestamp).total_seconds()\n",
    "        end_day = end_timestamp.weekday()\n",
    "        \n",
    "        #print start_timestamp, start_ts_roundup, start_seconds/3600.0, end_timestamp, end_ts_roundwn, end_seconds/3600.0\n",
    "        #day_counts = Counter(hour_periods.weekday)\n",
    "        day_periods = pd.date_range(start_ts_roundup, end_ts_roundwn, freq='D', closed='left')\n",
    "        day_counts2 = Counter(day_periods.weekday)\n",
    "        day_counts2[start_day] -= start_seconds/86400.0\n",
    "        day_counts2[end_day] -= end_seconds/86400.0\n",
    "        #for d_i in range(7):\n",
    "        #    assert abs(day_counts[d_i]/86400.0 - day_counts2[d_i]) < 10**-9\n",
    "        d[24] = (day_counts2[0] + day_counts2[1] + day_counts2[2] + day_counts2[3]) * 86400.0 / 1000.0\n",
    "        d[25] = day_counts2[4] * 86400.0/ 1000.0\n",
    "        d[26] = (day_counts2[5] + day_counts2[6]) * 86400.0 / 1000.0\n",
    "        \n",
    "        num_month_starts = np.sum(day_periods.is_month_start)\n",
    "        #print num_month_starts\n",
    "        if start_timestamp.is_month_start:\n",
    "            num_month_starts -= start_seconds/86400.0\n",
    "        if end_timestamp.is_month_start:\n",
    "            num_month_starts -= end_seconds/86400.0\n",
    "        #print start_timestamp, end_timestamp, np.sum(hour_periods.is_month_start), num_month_starts*86400.0\n",
    "        #assert abs(np.sum(hour_periods.is_month_start) - num_month_starts*86400.0) < 10**-9\n",
    "        #d[27] = np.sum(hour_periods.is_month_start) / 1000.0\n",
    "        d[27] = num_month_starts * 86400.0 / 1000.0\n",
    "        d = np.array(d)\n",
    "        #print d\n",
    "        \n",
    "        mu_term = np.sum(self._mu * d)\n",
    "        integral_value += mu_term\n",
    "        \n",
    "        return integral_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%pxlocal\n",
    "class SimpleMultivariateHawkesProcess:\n",
    "    _ndim = None # no. of dimensions\n",
    "    _lambda = None # ndim x 1 vector\n",
    "    _alpha = None # ndim x ndim matrix\n",
    "    _beta = None # ndim x ndim matrix\n",
    "    _observation_window = None\n",
    "    _EPS = 10**-9 # numbers below this are set to 0\n",
    "\n",
    "    def __init__(self, ndim=None, lam=None, alpha=None, beta=None, observation_window=None):\n",
    "        self._ndim = ndim\n",
    "        self._lambda = lam\n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        self._observation_window = observation_window\n",
    "    \n",
    "    def fit(self, ndim, alpha, data_epochs, data_dims, observation_window, niter=1000):\n",
    "        print 'Fitting...'\n",
    "        print '\\tPrecomputing A(i), B(i)...'\n",
    "        A, B = self._par_precompute(ndim, alpha, data_epochs, data_dims, observation_window)\n",
    "                \n",
    "        print '\\tMinimizing negative loglikelihood...'\n",
    "        alpha_ravel = alpha.ravel()\n",
    "        x0 = np.random.uniform(low=10**-5.0, high=alpha_ravel[0] - self._EPS, size=(ndim * (ndim + 1)))\n",
    "        bounds = [(self._EPS, None) for i in range(ndim)] +\\\n",
    "                 [(self._EPS, None) for i in range(ndim * ndim)] \n",
    "                 #[(self._EPS, None) for i in range(ndim * ndim)]\n",
    "                 \n",
    "                 \n",
    "            \n",
    "        x, f, d = scipy.optimize.fmin_l_bfgs_b(self._par_negloglikelihood,\n",
    "                                               x0=x0, args=(ndim, alpha, observation_window, A, B),\n",
    "                                               factr=10.0**7.0, iprint=99, pgtol=10**-5, approx_grad=False,\n",
    "                                               callback=self._callback, disp=99,\n",
    "                                               bounds=bounds, maxiter=niter, maxfun=niter)\n",
    "\n",
    "        return x, f, d\n",
    "    \n",
    "    def _callback(self, param):\n",
    "        print 'Optimization iteration done.'\n",
    "    \n",
    "    def _par_precompute(self, ndim, alpha, data_epochs, data_dims, observation_window):\n",
    "        nsamples = len(data_epochs)\n",
    "        v.execute('import numpy as np')\n",
    "        v.execute('import pandas as pd')\n",
    "        res = v.map_sync(self._precompute, [ndim] * nsamples, [alpha] * nsamples,\n",
    "                         [[d] for d in data_epochs], [[d] for d in data_dims], [observation_window] * nsamples)\n",
    "        A = [r[0][0] for r in res]\n",
    "        B = [r[1][0] for r in res]\n",
    "        return A, B\n",
    "    \n",
    "    @staticmethod\n",
    "    def _precompute(ndim, alpha, data_epochs, data_dims, observation_window):\n",
    "        T0, T = observation_window\n",
    "        nsamples = len(data_epochs)\n",
    "\n",
    "        A = []\n",
    "        B = [np.zeros(shape=(ndim, ndim), dtype=np.float64) for k in range(nsamples)]\n",
    "        for k in range(nsamples):\n",
    "            # memoize locations of timestamps for each dimension for this sample (2s for 18K samples)\n",
    "            epochs_per_dimension = []\n",
    "            for i in range(ndim):\n",
    "                idx_i = data_dims[k] == i\n",
    "                epochs_i = data_epochs[k][idx_i]\n",
    "                epochs_per_dimension.append(epochs_i)\n",
    "            \n",
    "            A_k = [np.zeros(shape=(ndim, len(epochs_per_dimension[m])), dtype=np.float64)\n",
    "                   for m in range(ndim)]\n",
    "            #B_mn = np.zeros(shape=(ndim, ndim), dtype=np.float64)\n",
    "            for m in range(ndim):\n",
    "                epochs_m = epochs_per_dimension[m]\n",
    "                if len(epochs_m) == 0:\n",
    "                    continue # no transactions on dimension m\n",
    "                #A_m = np.zeros(shape=(ndim, len(epochs_m)), dtype=np.float64)\n",
    "                for n in range(ndim):\n",
    "                    epochs_n = epochs_per_dimension[n]\n",
    "                    if len(epochs_n) == 0: # no transactions on dimension n\n",
    "                        continue\n",
    "                    len_n = len(epochs_n)\n",
    "                    A_mn = A_k[m][n] # influence of dimension n on each timestamp of dimension m\n",
    "\n",
    "                    j = 0\n",
    "                    if (m != n): # mutual excitation    \n",
    "                        \n",
    "                        while j < len_n and epochs_n[j] < epochs_m[0]:\n",
    "                            j += 1\n",
    "                            \n",
    "                        #A_mn[0] = np.sum(np.exp(-alpha[m,n] * (epochs_m[0] - epochs_n[epochs_n < epochs_m[0]]) / 1000.0))\n",
    "                        A_mn[0] = np.sum(np.exp(-alpha[m,n] * (epochs_m[0] - epochs_n[:j]) / 1000.0))\n",
    "\n",
    "                        for i in range(1, len(epochs_m)):\n",
    "                            prev_j = j\n",
    "                            while j < len_n and epochs_n[j] < epochs_m[i]:\n",
    "                                j += 1\n",
    "                            A_mn[i] = np.exp(-alpha[m,n] * (epochs_m[i] - epochs_m[i-1]) / 1000.0) * A_mn[i-1]\n",
    "                            #A_mn[i] += np.sum(np.exp(-alpha[m,n] *\n",
    "                            #                         (epochs_m[i] - epochs_n[(epochs_n >= epochs_m[i-1]) &\n",
    "                            #                                                 (epochs_n < epochs_m[i])]) / 1000.0))\n",
    "                            A_mn[i] += np.sum(np.exp(-alpha[m,n] *\n",
    "                                                     (epochs_m[i] - epochs_n[prev_j:j]) / 1000.0))\n",
    "                        \n",
    "                        B[k][m,n] = np.sum(1 - np.exp(-alpha[m,n] * (T - epochs_n) / 1000.0))\n",
    "                    else: # dim_m = dim_n, self-excitation\n",
    "                        for i in range(1, len(epochs_m)):\n",
    "                            A_mn[i] = np.exp(-alpha[m,n] * (epochs_m[i] - epochs_m[i-1]) / 1000.0) * (1 + A_mn[i-1])\n",
    "                        B[k][m,n] = np.sum(1 - np.exp(-alpha[m,n] * (T - epochs_m) / 1000.0))\n",
    "                \n",
    "                #A_k.append(A_m)\n",
    "            \n",
    "            A.append(A_k)\n",
    "            #B[k] = B_mn\n",
    "\n",
    "        return A, B\n",
    "    \n",
    "    def _par_negloglikelihood(self, params, ndim, alpha, observation_window, A, B):\n",
    "        nsamples = len(A)\n",
    "        res = v.map_sync(self._negloglikelihood, [params] * nsamples, [ndim] * nsamples,\n",
    "                         [alpha] * nsamples, [observation_window] * nsamples, A, B)\n",
    "        return np.sum([r[0] for r in res])/nsamples, np.sum(np.array([r[1] for r in res]), axis=0)/nsamples\n",
    "    \n",
    "    def _seq_negloglikelihood(self, params, ndim, alpha, observation_window, A, B):\n",
    "        nsamples = len(A)\n",
    "        nll = 0.0\n",
    "        grad = np.zeros(shape=((ndim + 1) * ndim), dtype=np.float64) # first row is grad of lambda\n",
    "        for k in range(nsamples):\n",
    "            sample_nll, sample_grad = self._negloglikelihood(params, ndim, alpha, observation_window, A[k], B[k])\n",
    "            nll += sample_nll\n",
    "            grad += sample_grad\n",
    "        nll /= nsamples\n",
    "        grad /= nsamples\n",
    "        return nll, grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _negloglikelihood(params, ndim, alpha, observation_window, A, B):\n",
    "        lam = params[:ndim] # first ndim elements\n",
    "        beta = params[ndim:].reshape((ndim, ndim)) # next ndim * ndim elements\n",
    "        T0, T = observation_window\n",
    "        T_length = (T - T0)/1000.0 # scale down\n",
    "        \n",
    "        loglikelihood = 0.0\n",
    "        grad_lambda = np.zeros(shape=(ndim), dtype=np.float64)\n",
    "        grad_beta = np.zeros(shape=(ndim, ndim), dtype=np.float64)\n",
    "\n",
    "        for m in range(ndim):\n",
    "            Z = lam[m] + np.sum(beta[m,:][:,None] * A[m], axis=0)\n",
    "            loglikelihood += np.sum(np.log(Z)) - np.sum(beta[m,:] * B[m,:] / alpha[m,:]) - lam[m] * T_length\n",
    "            grad_lambda[m] = np.sum(1.0/Z) - T_length\n",
    "            grad_beta[m] = np.sum(A[m]/Z, axis=1) - B[m,:]/alpha[m,:]\n",
    "            \n",
    "            #print np.sum(np.log(Z)) - np.sum(beta[m,:] * B[m,:] / alpha[m,:]) - lam[m] * T_length \\\n",
    "            #   - len(A[m][0]) * np.log(1000) # compare with Nan\n",
    "            #print 1000.0 * np.sum(1.0/Z) - T_length * 1000.0 # compare with Nan\n",
    "            #print 1000.0 * grad_beta[m] # compare with nan\n",
    "        \n",
    "        gradient = np.concatenate((grad_lambda, grad_beta.ravel()))\n",
    "        return -loglikelihood, -gradient\n",
    "\n",
    "    def check_gradient(self, ndim, alpha, data_epochs, data_dims, observation_window, delta=10**-9, niter=50):\n",
    "        print 'Checking gradient...'\n",
    "        print '\\tPrecomputing A, B...'\n",
    "        A, B = self._precompute(ndim, alpha, data_epochs, data_dims, observation_window)\n",
    "        \n",
    "        nsamples = len(data_epochs)\n",
    "        for i in range(niter):\n",
    "            row_idx = np.random.randint(0, nsamples)\n",
    "            a = A[row_idx]\n",
    "            b = B[row_idx]\n",
    "            x0 = np.array([np.random.uniform(10**-3, 10**3), np.random.uniform(10**-3, 10**3),  # lambda\n",
    "                           np.random.uniform(10**-3, 10**3), np.random.uniform(10**-3, 10**3),  # beta\n",
    "                           np.random.uniform(10**-3, 10**3), np.random.uniform(10**-3, 10**3)]) # beta\n",
    "\n",
    "            nll, grad = self._negloglikelihood(x0, ndim, alpha, observation_window, a, b)\n",
    "            param_idx = np.random.randint(len(x0))\n",
    "            x0[param_idx] += delta\n",
    "            new_nll, new_grad = self._negloglikelihood(x0, ndim, alpha, observation_window, a, b)\n",
    "            num_grad = (new_nll - nll)/delta            \n",
    "            if abs((grad[param_idx] - num_grad)/num_grad)  > 0.01:\n",
    "                print 'Gradient checking failed for parameter:', param_idx\n",
    "                print 'Analytical gradient:', grad[param_idx],\n",
    "                print 'Numerical gradient:', num_grad\n",
    "        print '\\tGradient checking complete.'\n",
    "        \n",
    "    def simulate(self, t0, prev_t, prev_d, horizon_duration):\n",
    "        simulated_iats = []\n",
    "        simulated_dims = []\n",
    "        prev_t_copy = copy.copy(list(prev_t))\n",
    "        prev_d_copy = copy.copy(list(prev_d))\n",
    "        prev_t_array = np.array(prev_t_copy, dtype=np.float64)\n",
    "        prev_d_array = np.array(prev_d_copy, dtype=np.int)\n",
    "        cumulative_iat = 0.0\n",
    "        while cumulative_iat < horizon_duration:\n",
    "            upper_bound = np.sum(self.intensity(t0 + cumulative_iat, (prev_t_array, prev_d_array), include_t0=True))\n",
    "            s = 1000.0 * np.random.exponential(1.0/upper_bound) # params are in 1/1000s units            \n",
    "                \n",
    "            cumulative_iat += s # move ahead in time (may accept or reject)\n",
    "            if cumulative_iat >= horizon_duration:\n",
    "                break\n",
    "\n",
    "            lambda_t_dim = self.intensity(t0 + cumulative_iat, (prev_t_array, prev_d_array))\n",
    "            lambda_t = np.sum(lambda_t_dim)\n",
    "\n",
    "            u = np.random.uniform(0.0, 1.0)\n",
    "            if u <= lambda_t/upper_bound: # accept\n",
    "                prev_t_copy.append(t0 + cumulative_iat)\n",
    "                prev_t_array = np.array(prev_t_copy, dtype=np.float64)\n",
    "                simulated_iats.append(cumulative_iat)\n",
    "                \n",
    "                cumprob = np.cumsum(lambda_t_dim/lambda_t)\n",
    "                u2 = np.random.uniform(0.0, 1.0)\n",
    "                dim = np.searchsorted(cumprob, u2)\n",
    "                \n",
    "                prev_d_copy.append(dim)\n",
    "                prev_d_array = np.array(prev_d_copy, dtype=np.int)\n",
    "                simulated_dims.append(dim)\n",
    "\n",
    "        return simulated_iats, simulated_dims\n",
    "    \n",
    "    def predict_dist(self, t0, prev_t, prev_d, niter=1000):\n",
    "        #step = 3600.0 - (t0 % 3600)\n",
    "        simulated_iats = np.zeros(shape=(niter), dtype=np.float64)\n",
    "        simulated_dims = np.zeros(shape=(niter), dtype=np.int)\n",
    "        for i in range(niter):\n",
    "            cumulative_iat = 0.0\n",
    "            while True:\n",
    "                upper_bound = np.sum(self.intensity(t0 + cumulative_iat, (prev_t, prev_d), include_t0=True))\n",
    "                s = 1000.0 * np.random.exponential(1.0/upper_bound) # params are in 1/1000s units\n",
    "\n",
    "                #print 'Candidate next IAT:', t + s\n",
    "                #if s >= step: # upper bound no longer valid\n",
    "                #    t += step\n",
    "                #    step = 3600.0 # one hour in seconds\n",
    "                #    #print '\\tLarger than step, moved to:', t\n",
    "                #    continue\n",
    "                \n",
    "                cumulative_iat += s # move ahead in time (may accept or reject)\n",
    "                #step -= s\n",
    "                lambda_t_dim = self.intensity(t0 + cumulative_iat, (prev_t, prev_d))\n",
    "                lambda_t = np.sum(lambda_t_dim)\n",
    "                u = np.random.uniform(0.0, 1.0)\n",
    "                \n",
    "                if u <= lambda_t/upper_bound: # accept\n",
    "                    simulated_iats[i] = cumulative_iat\n",
    "                    if self._ndim > 1:\n",
    "                        simulated_dims[i] = np.random.choice(range(self._ndim), p=lambda_t_dim/lambda_t)\n",
    "                    break\n",
    "    \n",
    "        return simulated_iats, simulated_dims\n",
    "\n",
    "    def intensity(self, t0, prev_e, include_t0=False):\n",
    "        prev_t, prev_d = prev_e\n",
    "        if not include_t0:\n",
    "            prev_t_ = prev_t[prev_t < t0]\n",
    "            prev_d_ = prev_d[prev_t < t0]\n",
    "        else:\n",
    "            prev_t_ = prev_t[prev_t <= t0]\n",
    "            prev_d_ = prev_d[prev_t <= t0]\n",
    "        t0_minus_ti = (t0 - prev_t_) / 1000.0\n",
    "        return self._lambda + np.sum(self._beta[:, prev_d_] * np.exp(-self._alpha[:, prev_d_] * t0_minus_ti), axis=1)\n",
    "    \n",
    "    def intensity_integral(self, start_t, end_t, data_epochs, data_dims):\n",
    "        # memoize locations of timestamps for each dimension for this sample (2s for 18K samples)\n",
    "        epochs_per_dimension = []\n",
    "        for i in range(self._ndim):\n",
    "            idx_i = data_dims == i\n",
    "            epochs_i = data_epochs[idx_i]\n",
    "            epochs_per_dimension.append(epochs_i)\n",
    "        \n",
    "        integral_value = 0.0\n",
    "        \n",
    "        for m in range(self._ndim):\n",
    "            integral_value += self._lambda[m] * (end_t - start_t) / 1000.0\n",
    "            for n in range(self._ndim):\n",
    "                beta_mn = self._beta[m,n]\n",
    "                alpha_mn = self._alpha[m,n]\n",
    "                epochs_n = epochs_per_dimension[n]\n",
    "                \n",
    "                epochs_n_ = epochs_n[epochs_n < start_t]\n",
    "                term1 = np.sum(np.exp(-alpha_mn * (start_t - epochs_n_) / 1000.0) -\n",
    "                               np.exp(-alpha_mn * (end_t - epochs_n_) / 1000.0))\n",
    "                \n",
    "                epochs_n_ = epochs_n[(epochs_n >= start_t) & (epochs_n < end_t)]\n",
    "                term2 = np.sum(1 - np.exp(-alpha_mn * (end_t - epochs_n_) / 1000.0))\n",
    "                \n",
    "                integral_value += beta_mn/alpha_mn * (term1 + term2)\n",
    "        \n",
    "        return integral_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%pxlocal\n",
    "class TimeVaryingMultivariateHawkesProcess:\n",
    "    _ndim = None # no. of dimensions\n",
    "    _lambda = None # ndim x 1 vector\n",
    "    _alpha = None # ndim x ndim matrix\n",
    "    _beta = None # ndim x ndim matrix\n",
    "    _mu = None # ndim x nfeatures matrix\n",
    "    _observation_window = None\n",
    "    _EPS = 10**-9 # numbers below this are set to 0\n",
    "\n",
    "    def __init__(self, ndim=None, lam=None, alpha=None, beta=None, mu=None, observation_window=None):\n",
    "        self._ndim = ndim\n",
    "        self._lambda = lam\n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        self._mu = mu\n",
    "        self._observation_window = observation_window\n",
    "    \n",
    "    def fit(self, ndim, alpha, F, D, data_epochs, data_dims, observation_window, niter=1000):\n",
    "        print 'Fitting...'\n",
    "        print '\\tPrecomputing A(i), B(i)...'\n",
    "        A, B = self._par_recompute(ndim, alpha, data_epochs, data_dims, observation_window)\n",
    "                \n",
    "        print '\\tMinimizing negative loglikelihood...'\n",
    "        alpha_ravel = alpha.ravel()\n",
    "        nfeatures = D.shape[0]\n",
    "        x0 = np.random.uniform(low=10**-5.0, high=alpha_ravel[0] - self._EPS, size=(ndim * (ndim + 1) + ndim*nfeatures))\n",
    "        bounds = [(self._EPS, None) for i in range(ndim)] +\\\n",
    "                 [(self._EPS, None) for i in range(ndim * ndim)] +\\\n",
    "                 [(self._EPS, None) for i in range(ndim * nfeatures)]\n",
    "                 #[(self._EPS, None) for i in range(ndim * ndim)]\n",
    "                 \n",
    "        x, f, d = scipy.optimize.fmin_l_bfgs_b(self._par_negloglikelihood,\n",
    "                                               x0=x0, args=(ndim, alpha, data_dims, F, D, observation_window, A, B),\n",
    "                                               factr=10.0**7.0, iprint=99, pgtol=10**-5, approx_grad=False,\n",
    "                                               disp=99,\n",
    "                                               bounds=bounds, callback=self._callback, maxiter=niter, maxfun=niter)\n",
    "\n",
    "        return x, f, d\n",
    "    \n",
    "    def _par_precompute(self, ndim, alpha, data_epochs, data_dims, observation_window):\n",
    "        nsamples = len(data_epochs)\n",
    "        v.execute('import numpy as np')\n",
    "        v.execute('import pandas as pd')\n",
    "        res = v.map_sync(self._precompute, [ndim] * nsamples, [alpha] * nsamples,\n",
    "                         [[d] for d in data_epochs], [[d] for d in data_dims], [observation_window] * nsamples)\n",
    "        A = [r[0][0] for r in res]\n",
    "        B = [r[1][0] for r in res]\n",
    "        return A, B\n",
    "    \n",
    "    @staticmethod\n",
    "    def _precompute(ndim, alpha, data_epochs, data_dims, observation_window):\n",
    "        T0, T = observation_window\n",
    "        nsamples = len(data_epochs)\n",
    "\n",
    "        A = []\n",
    "        B = [np.zeros(shape=(ndim, ndim), dtype=np.float64) for k in range(nsamples)]\n",
    "        for k in range(nsamples):\n",
    "            # memoize locations of timestamps for each dimension for this sample (2s for 18K samples)\n",
    "            epochs_per_dimension = []\n",
    "            for i in range(ndim):\n",
    "                idx_i = data_dims[k] == i\n",
    "                epochs_i = data_epochs[k][idx_i]\n",
    "                epochs_per_dimension.append(epochs_i)\n",
    "            \n",
    "            A_k = [np.zeros(shape=(ndim, len(epochs_per_dimension[m])), dtype=np.float64)\n",
    "                   for m in range(ndim)]\n",
    "            #B_mn = np.zeros(shape=(ndim, ndim), dtype=np.float64)\n",
    "            for m in range(ndim):\n",
    "                epochs_m = epochs_per_dimension[m]\n",
    "                if len(epochs_m) == 0:\n",
    "                    continue # no transactions on dimension m\n",
    "                #A_m = np.zeros(shape=(ndim, len(epochs_m)), dtype=np.float64)\n",
    "                for n in range(ndim):\n",
    "                    epochs_n = epochs_per_dimension[n]\n",
    "                    if len(epochs_n) == 0: # no transactions on dimension n\n",
    "                        continue\n",
    "                    len_n = len(epochs_n)\n",
    "                    A_mn = A_k[m][n] # influence of dimension n on each timestamp of dimension m\n",
    "\n",
    "                    j = 0\n",
    "                    \n",
    "                    if (m != n): # mutual excitation    \n",
    "                        \n",
    "                        while j < len_n and epochs_n[j] < epochs_m[0]:\n",
    "                            j += 1\n",
    "                            \n",
    "                        #A_mn[0] = np.sum(np.exp(-alpha[m,n] * (epochs_m[0] - epochs_n[epochs_n < epochs_m[0]]) / 1000.0))\n",
    "                        A_mn[0] = np.sum(np.exp(-alpha[m,n] * (epochs_m[0] - epochs_n[:j]) / 1000.0))\n",
    "\n",
    "                        for i in range(1, len(epochs_m)):\n",
    "                            prev_j = j\n",
    "                            while j < len_n and epochs_n[j] < epochs_m[i]:\n",
    "                                j += 1\n",
    "                            A_mn[i] = np.exp(-alpha[m,n] * (epochs_m[i] - epochs_m[i-1]) / 1000.0) * A_mn[i-1]\n",
    "                            #A_mn[i] += np.sum(np.exp(-alpha[m,n] *\n",
    "                            #                         (epochs_m[i] - epochs_n[(epochs_n >= epochs_m[i-1]) &\n",
    "                            #                                                 (epochs_n < epochs_m[i])]) / 1000.0))\n",
    "                            A_mn[i] += np.sum(np.exp(-alpha[m,n] *\n",
    "                                                     (epochs_m[i] - epochs_n[prev_j:j]) / 1000.0))\n",
    "                        \n",
    "                        B[k][m,n] = np.sum(1 - np.exp(-alpha[m,n] * (T - epochs_n) / 1000.0))\n",
    "                    else: # dim_m = dim_n, self-excitation\n",
    "                        for i in range(1, len(epochs_m)):\n",
    "                            A_mn[i] = np.exp(-alpha[m,n] * (epochs_m[i] - epochs_m[i-1]) / 1000.0) * (1 + A_mn[i-1])\n",
    "                        B[k][m,n] = np.sum(1 - np.exp(-alpha[m,n] * (T - epochs_m) / 1000.0))\n",
    "                \n",
    "                #A_k.append(A_m)\n",
    "            \n",
    "            A.append(A_k)\n",
    "            #B[k] = B_mn\n",
    "\n",
    "        return A, B\n",
    "\n",
    "    def _callback(self, param):\n",
    "        print '.'\n",
    "    \n",
    "    def _par_negloglikelihood(self, params, ndim, alpha, data_dims, F, D, observation_window, A, B):\n",
    "        nsamples = len(A)\n",
    "        nfeatures = D.shape[0]\n",
    "        nll = 0.0\n",
    "        grad = np.zeros(shape=((ndim + 1) * ndim + ndim*nfeatures), dtype=np.float64)\n",
    "        res = v.map_sync(self._negloglikelihood, [params] * nsamples, [ndim] * nsamples,\n",
    "                         [alpha] * nsamples, data_dims, F, [D] * nsamples, [observation_window] * nsamples, A, B)\n",
    "        nll = np.sum([r[0] for r in res])/float(nsamples)\n",
    "        grad = np.sum(np.array([r[1] for r in res]), axis=0)/nsamples\n",
    "        return nll, grad\n",
    "    \n",
    "    def _seq_negloglikelihood(self, params, ndim, alpha, data_dims, F, D, observation_window, A, B):\n",
    "        nsamples = len(A)\n",
    "        nfeatures = D.shape[0]\n",
    "        nll = 0.0\n",
    "        grad = np.zeros(shape=((ndim + 1) * ndim + ndim*nfeatures), dtype=np.float64) # first row is grad of lambda\n",
    "        for k in range(nsamples):\n",
    "            sample_nll, sample_grad = self._negloglikelihood(params, ndim, alpha, data_dims[k],\n",
    "                                                             F[k], D, observation_window, A[k], B[k])\n",
    "            nll += sample_nll\n",
    "            grad += sample_grad\n",
    "        nll /= nsamples\n",
    "        grad /= nsamples\n",
    "        return nll, grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def _negloglikelihood(params, ndim, alpha, data_dims, F, D, observation_window, A, B):\n",
    "        nfeatures = D.shape[0]\n",
    "        lam = params[:ndim] # first ndim elements\n",
    "        beta = params[ndim:ndim+ndim*ndim].reshape((ndim, ndim)) # next ndim * ndim elements\n",
    "        mu = params[ndim+ndim*ndim:ndim+ndim*ndim + ndim*nfeatures].reshape((ndim,nfeatures))\n",
    "        T0, T = observation_window\n",
    "        T_length = (T - T0)/1000.0 # scale down\n",
    "        \n",
    "        loglikelihood = 0.0\n",
    "        grad_lambda = np.zeros(shape=(ndim), dtype=np.float64)\n",
    "        grad_beta = np.zeros(shape=(ndim, ndim), dtype=np.float64)\n",
    "        grad_mu = np.zeros(shape=(ndim, nfeatures), dtype=np.float64)\n",
    "\n",
    "        for m in range(ndim):\n",
    "            mu_m = mu[m,:]\n",
    "            dimindex = data_dims == m\n",
    "            F_m = F[:,dimindex]\n",
    "            C_m = np.dot(mu_m, F_m)\n",
    "\n",
    "            Z = lam[m] + np.sum(beta[m,:][:,None] * A[m], axis=0)\n",
    "            Z += C_m\n",
    "            loglikelihood += np.sum(np.log(Z)) - np.sum(beta[m,:] * B[m,:] / alpha[m,:]) - lam[m] * T_length\n",
    "            loglikelihood -= np.sum(mu_m * D)\n",
    "            grad_lambda[m] = np.sum(1.0/Z) - T_length\n",
    "            grad_beta[m] = np.sum(A[m]/Z, axis=1) - B[m,:]/alpha[m,:]\n",
    "            grad_mu[m] = [np.sum(F_m[j]/Z) - D[j] for j in range(nfeatures)]\n",
    "        \n",
    "        gradient = np.concatenate((grad_lambda, grad_beta.ravel(), grad_mu.ravel()))\n",
    "        return -loglikelihood, -gradient\n",
    "\n",
    "    def check_gradient(self, ndim, alpha, data_epochs, data_dims, F, D, observation_window, delta=10**-8, niter=50):\n",
    "        print 'Checking gradient...'\n",
    "        print '\\tPrecomputing A, B...'\n",
    "        A, B = self._precompute(ndim, alpha, data_epochs, data_dims, observation_window)\n",
    "        \n",
    "        nfeatures = D.shape[0]\n",
    "        nsamples = len(data_epochs)\n",
    "        for i in range(niter):\n",
    "            row_idx = np.random.randint(0, nsamples)\n",
    "            a = A[row_idx]\n",
    "            b = B[row_idx]\n",
    "            f = F[row_idx]\n",
    "            d = data_dims[row_idx]\n",
    "            alpha_ravel = alpha.ravel()\n",
    "            x0 = np.random.uniform(low=10**-5.0, high=alpha_ravel[0] - self._EPS,\n",
    "                                   size=(ndim * (ndim + 1) + ndim*nfeatures))\n",
    "            nll, grad = self._negloglikelihood(x0, ndim, alpha, d, f, D, observation_window, a, b)\n",
    "            param_idx = np.random.randint(len(x0))\n",
    "            x0[param_idx] += delta\n",
    "            new_nll, new_grad = self._negloglikelihood(x0, ndim, alpha, d, f, D, observation_window, a, b)\n",
    "            num_grad = (new_nll - nll)/delta            \n",
    "            if abs((grad[param_idx] - num_grad)/num_grad)  > 0.01:\n",
    "                print 'Gradient checking failed for parameter:', param_idx\n",
    "                print 'Analytical gradient:', grad[param_idx],\n",
    "                print 'Numerical gradient:', num_grad\n",
    "        print '\\tGradient checking complete.'\n",
    "        \n",
    "    def simulate(self, t0, prev_t, prev_d, horizon_duration):\n",
    "        simulated_iats = []\n",
    "        simulated_dims = []\n",
    "        prev_t_copy = copy.copy(list(prev_t))\n",
    "        prev_d_copy = copy.copy(list(prev_d))\n",
    "        prev_t_array = np.array(prev_t_copy, dtype=np.float64)\n",
    "        prev_d_array = np.array(prev_d_copy, dtype=np.int)\n",
    "        cumulative_iat = 0.0\n",
    "        while cumulative_iat < horizon_duration:\n",
    "            upper_bound = np.sum(self.intensity(t0 + cumulative_iat, (prev_t_array, prev_d_array), include_t0=True))\n",
    "            s = 1000.0 * np.random.exponential(1.0/upper_bound) # params are in 1/1000s units            \n",
    "                \n",
    "            cumulative_iat += s # move ahead in time (may accept or reject)\n",
    "            if cumulative_iat >= horizon_duration:\n",
    "                break\n",
    "\n",
    "            lambda_t_dim = self.intensity(t0 + cumulative_iat, (prev_t_array, prev_d_array))\n",
    "            lambda_t = np.sum(lambda_t_dim)\n",
    "\n",
    "            u = np.random.uniform(0.0, 1.0)\n",
    "            if u <= lambda_t/upper_bound: # accept\n",
    "                prev_t_copy.append(t0 + cumulative_iat)\n",
    "                prev_t_array = np.array(prev_t_copy, dtype=np.float64)\n",
    "                simulated_iats.append(cumulative_iat)\n",
    "                \n",
    "                cumprob = np.cumsum(lambda_t_dim/lambda_t)\n",
    "                u2 = np.random.uniform(0.0, 1.0)\n",
    "                dim = np.searchsorted(cumprob, u2)\n",
    "                \n",
    "                prev_d_copy.append(dim)\n",
    "                prev_d_array = np.array(prev_d_copy, dtype=np.int)\n",
    "                simulated_dims.append(dim)\n",
    "\n",
    "        return simulated_iats, simulated_dims\n",
    "    \n",
    "    def predict_dist(self, t0, prev_t, prev_d, niter=1000):\n",
    "        step = 3600.0 - (t0 % 3600)\n",
    "        simulated_iats = np.zeros(shape=(niter), dtype=np.float64)\n",
    "        simulated_dims = np.zeros(shape=(niter), dtype=np.int)\n",
    "        for i in range(niter):\n",
    "            cumulative_iat = 0.0\n",
    "            while True:\n",
    "                #print 'trial start'\n",
    "                upper_bound = np.sum(self.intensity(t0 + cumulative_iat, (prev_t, prev_d), include_t0=True))\n",
    "                s = 1000.0 * np.random.exponential(1.0/upper_bound) # params are in 1/1000s units\n",
    "\n",
    "                #print 'Candidate next IAT:', t + s\n",
    "                if s >= step: # upper bound no longer valid\n",
    "                    cumulative_iat += step\n",
    "                    step = 3600.0 # one hour in seconds\n",
    "                    #print '\\tLarger than step, moved to:', t\n",
    "                    continue\n",
    "                \n",
    "                cumulative_iat += s # move ahead in time (may accept or reject)\n",
    "                step -= s\n",
    "                lambda_t_dim = self.intensity(t0 + cumulative_iat, (prev_t, prev_d))\n",
    "                lambda_t = np.sum(lambda_t_dim)\n",
    "                u = np.random.uniform(0.0, 1.0)\n",
    "                \n",
    "                if u <= lambda_t/upper_bound: # accept\n",
    "                    simulated_iats[i] = cumulative_iat\n",
    "                    if self._ndim > 1:\n",
    "                        #print 'simulating dim'\n",
    "                        simulated_dims[i] = np.random.choice(range(self._ndim), p=lambda_t_dim/lambda_t)\n",
    "                        #print 'done simulating dim'\n",
    "                    break\n",
    "    \n",
    "        return simulated_iats, simulated_dims\n",
    "\n",
    "    def intensity(self, t0, prev_e, include_t0=False):\n",
    "        prev_t, prev_d = prev_e\n",
    "        if not include_t0:\n",
    "            prev_t_ = prev_t[prev_t < t0]\n",
    "            prev_d_ = prev_d[prev_t < t0]\n",
    "        else:\n",
    "            prev_t_ = prev_t[prev_t <= t0]\n",
    "            prev_d_ = prev_d[prev_t <= t0]\n",
    "        \n",
    "        t0_minus_ti = (t0 - prev_t_) / 1000.0\n",
    "        intensity = self._lambda + np.sum(self._beta[:, prev_d_] * np.exp(-self._alpha[:, prev_d_] * t0_minus_ti), axis=1)\n",
    "        \n",
    "        t0_ = time.gmtime(t0)\n",
    "        hour_of_day, day_of_week, day_of_month = map(int, time.strftime(\"%H %w %d\", t0_).split(' '))\n",
    "        day_of_week = (day_of_week - 1) % 7\n",
    "        \n",
    "        f = np.array([int(hour_of_day == h) for h in range(24)] +\n",
    "                     [int(day_of_week >= 0 and day_of_week <= 3)] +\n",
    "                     [int(day_of_week == 4)] +\n",
    "                     [int(day_of_week >= 5 and day_of_week <= 6)] +\n",
    "                     [int(day_of_month == 1)], dtype=np.int)\n",
    "\n",
    "        #print intensity.shape, self._mu.shape, f.shape, (self._mu * f).shape, np.sum(self._mu * f, axis=0).shape\n",
    "        intensity += np.sum(self._mu * f, axis=1)\n",
    "        \n",
    "        return intensity\n",
    "    \n",
    "    def intensity_integral(self, start_t, end_t, data_epochs, data_dims):\n",
    "        # memoize locations of timestamps for each dimension for this sample (2s for 18K samples)\n",
    "        epochs_per_dimension = []\n",
    "        for i in range(self._ndim):\n",
    "            idx_i = data_dims == i\n",
    "            epochs_i = data_epochs[idx_i]\n",
    "            epochs_per_dimension.append(epochs_i)\n",
    "        \n",
    "        integral_value = 0.0\n",
    "        \n",
    "        for m in range(self._ndim):\n",
    "            integral_value += self._lambda[m] * (end_t - start_t) / 1000.0\n",
    "            for n in range(self._ndim):\n",
    "                beta_mn = self._beta[m,n]\n",
    "                alpha_mn = self._alpha[m,n]\n",
    "                epochs_n = epochs_per_dimension[n]\n",
    "                \n",
    "                epochs_n_ = epochs_n[epochs_n < start_t]\n",
    "                term1 = np.sum(np.exp(-alpha_mn * (start_t - epochs_n_) / 1000.0) -\n",
    "                               np.exp(-alpha_mn * (end_t - epochs_n_) / 1000.0))\n",
    "                \n",
    "                epochs_n_ = epochs_n[(epochs_n >= start_t) & (epochs_n < end_t)]\n",
    "                term2 = np.sum(1 - np.exp(-alpha_mn * (end_t - epochs_n_) / 1000.0))\n",
    "                \n",
    "                integral_value += beta_mn/alpha_mn * (term1 + term2)\n",
    "        \n",
    "        # mu term integral\n",
    "        d = np.zeros(28, dtype=np.float64)\n",
    "        start_timestamp = pd.to_datetime(start_t, unit='s')\n",
    "        end_timestamp = pd.to_datetime(end_t, unit='s')\n",
    "\n",
    "        #print 't_i-1', start_timestamp, 't_i', end_timestamp\n",
    "        \n",
    "        start_ts_roundup = start_timestamp.floor(freq='H')\n",
    "        start_seconds = (start_timestamp - start_ts_roundup).total_seconds()\n",
    "        start_hour = start_timestamp.hour\n",
    "        end_ts_roundwn = end_timestamp.ceil(freq='H')\n",
    "        end_seconds = (end_ts_roundwn - end_timestamp).total_seconds()\n",
    "        end_hour = end_timestamp.hour\n",
    "\n",
    "        #print 't_i-1', start_timestamp, 't_i', end_timestamp\n",
    "        \n",
    "        #hour_periods = pd.date_range(start_timestamp, end_timestamp, freq='s', closed='left')\n",
    "        hour_periods2 = pd.date_range(start_ts_roundup, end_ts_roundwn, freq='H', closed='left')\n",
    "        #print hour_periods\n",
    "        #hour_counts = Counter(hour_periods.hour)\n",
    "        hour_counts2 = Counter(hour_periods2.hour)\n",
    "        hour_counts2[start_hour] -= start_seconds/3600.0\n",
    "        hour_counts2[end_hour] -= end_seconds/3600.0\n",
    "        #print hour_counts2\n",
    "        #print start_timestamp, start_ts_roundup, end_timestamp, end_ts_roundwn\n",
    "        for h in range(24):\n",
    "            #d[h] = hour_counts[h] / 1000.0\n",
    "            d[h] = (hour_counts2[h] * 3600.0) / 1000.0\n",
    "            #print h, hour_counts[h], hour_counts2[h] * 3600.0\n",
    "            #assert abs(d[h] - (hour_counts2[h] * 3600.0) / 1000.0) < 10**-9\n",
    "        \n",
    "        start_ts_roundup = start_timestamp.floor(freq='D')\n",
    "        start_seconds = (start_timestamp - start_ts_roundup).total_seconds()\n",
    "        start_day = start_timestamp.weekday()\n",
    "        end_ts_roundwn = end_timestamp.ceil(freq='D')\n",
    "        end_seconds = (end_ts_roundwn - end_timestamp).total_seconds()\n",
    "        end_day = end_timestamp.weekday()\n",
    "        \n",
    "        #print start_timestamp, start_ts_roundup, start_seconds/3600.0, end_timestamp, end_ts_roundwn, end_seconds/3600.0\n",
    "        #day_counts = Counter(hour_periods.weekday)\n",
    "        day_periods = pd.date_range(start_ts_roundup, end_ts_roundwn, freq='D', closed='left')\n",
    "        day_counts2 = Counter(day_periods.weekday)\n",
    "        day_counts2[start_day] -= start_seconds/86400.0\n",
    "        day_counts2[end_day] -= end_seconds/86400.0\n",
    "        #for d_i in range(7):\n",
    "        #    assert abs(day_counts[d_i]/86400.0 - day_counts2[d_i]) < 10**-9\n",
    "        d[24] = (day_counts2[0] + day_counts2[1] + day_counts2[2] + day_counts2[3]) * 86400.0 / 1000.0\n",
    "        d[25] = day_counts2[4] * 86400.0/ 1000.0\n",
    "        d[26] = (day_counts2[5] + day_counts2[6]) * 86400.0 / 1000.0\n",
    "        \n",
    "        num_month_starts = np.sum(day_periods.is_month_start)\n",
    "        #print num_month_starts\n",
    "        if start_timestamp.is_month_start:\n",
    "            num_month_starts -= start_seconds/86400.0\n",
    "        if end_timestamp.is_month_start:\n",
    "            num_month_starts -= end_seconds/86400.0\n",
    "        #print start_timestamp, end_timestamp, np.sum(hour_periods.is_month_start), num_month_starts*86400.0\n",
    "        #assert abs(np.sum(hour_periods.is_month_start) - num_month_starts*86400.0) < 10**-9\n",
    "        #d[27] = np.sum(hour_periods.is_month_start) / 1000.0\n",
    "        d[27] = num_month_starts * 86400.0 / 1000.0\n",
    "        d = np.array(d)\n",
    "        #print d\n",
    "        \n",
    "        mu_term = np.sum(self._mu * d)\n",
    "        integral_value += mu_term\n",
    "        \n",
    "        return integral_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%pxlocal\n",
    "class TimeVaryingHawkesProcess:\n",
    "    _lambda = None\n",
    "    _alpha = None\n",
    "    _beta = None\n",
    "    _mu = None\n",
    "    _observation_window = None\n",
    "    _features=None\n",
    "    _EPS = 10**-9 # numbers below this are set to 0\n",
    "\n",
    "    def __init__(self, lam=None, alpha=None, mu=None, beta=None, observation_window=None):\n",
    "        self._lambda = lam\n",
    "        self._alpha = alpha\n",
    "        self._mu = mu\n",
    "        self._beta = beta\n",
    "        self._observation_window = observation_window\n",
    "    \n",
    "    def fit(self, alpha, F, D, data, observation_window, niter=1000):\n",
    "        print '\\tPrecomputing A(i), B(i)...'\n",
    "        A, B = self._precompute(alpha, data, observation_window)\n",
    "                \n",
    "        print 'Fitting...'\n",
    "        x0 = np.concatenate((np.random.uniform(10**-4, 1.0, size=D.shape[0]+1),\n",
    "                             np.random.uniform(self._EPS, alpha-self._EPS, size=1)), axis=0) \n",
    "        bounds = [(self._EPS, None)] * (D.shape[0] + 1) + [(self._EPS, alpha - self._EPS)]\n",
    "            \n",
    "        x, f, d = scipy.optimize.fmin_l_bfgs_b(self._seq_negloglikelihood,\n",
    "                                               x0=x0,\n",
    "                                               args=(alpha, F, D, data, observation_window, A, B),\n",
    "                                               factr=10.0**3.0, iprint=99, pgtol=10**-12, approx_grad=False,\n",
    "                                               disp=99, bounds=bounds, callback=None, maxiter=niter, maxfun=niter)\n",
    "\n",
    "        return x, f, d\n",
    "    \n",
    "    def _par_precompute(self, alpha, data_epochs, observation_window):\n",
    "        nsamples = len(data_epochs)\n",
    "        v.execute('import numpy as np')\n",
    "        v.execute('import pandas as pd')\n",
    "        res = v.map_sync(self._precompute, [alpha] * nsamples,\n",
    "                         [[d] for d in data_epochs], [observation_window] * nsamples)\n",
    "        A = [r[0][0] for r in res]\n",
    "        B = [r[1][0] for r in res]\n",
    "        return A, B\n",
    "    \n",
    "    @staticmethod\n",
    "    def _precompute(alpha, data_epochs, observation_window):\n",
    "        T0, T = observation_window\n",
    "        A = [np.zeros(len(sequence), dtype=np.float64) for sequence in data_epochs]\n",
    "        B = np.zeros(len(data_epochs), dtype=np.float64) # intensity integral sum, 1/alpha * \\sum_t 1 - exp(T - t)\n",
    "        for idx, sequence in enumerate(data_epochs): \n",
    "            for i in range(1, len(sequence)):\n",
    "                A[idx][i] = np.exp(-alpha * (sequence[i] - sequence[i-1]) / 1000.0) * (1 + A[idx][i-1])\n",
    "            B[idx] = np.sum(1 - np.exp(-alpha * (T - sequence) / 1000.0))\n",
    "        return A, B\n",
    "\n",
    "    def _callback(self, param):\n",
    "        print param\n",
    "    \n",
    "    def _seq_negloglikelihood(self, params, alpha, F, D, data, observation_window, A, B):\n",
    "        nsamples = len(data)\n",
    "        ll = []\n",
    "        gr = []\n",
    "        for i in range(nsamples):\n",
    "            ll_i, gr_i = self._negloglikelihood(params, alpha, F[i], D, data[i], observation_window, A[i], B[i])\n",
    "            ll.append(ll_i)\n",
    "            gr.append(gr_i)\n",
    "        return np.sum(ll)/nsamples, np.sum(np.array(gr), axis=0)/nsamples\n",
    "\n",
    "    @staticmethod\n",
    "    def _negloglikelihood(params, alpha, F, D, sequence, observation_window, A, B):\n",
    "        lam = params[0]\n",
    "        mu = params[1:-1]\n",
    "        beta = params[-1]\n",
    "        T0, T = observation_window\n",
    "        T_length = (T - T0)/1000.0 # scale down\n",
    "        \n",
    "        loglikelihood = 0.0\n",
    "        grad_lambda = 0.0\n",
    "        grad_beta = 0.0\n",
    "        grad_mu = [0.0] * mu.shape[0]\n",
    "\n",
    "        C = np.dot(mu, F) # C(i) = \\sum_j mu_j f_j(t_i)\n",
    "        Z = lam + beta * A + C\n",
    "        loglikelihood = np.sum(np.log(Z)) - beta/alpha * B - lam * T_length - np.sum(mu * D) # D is already in 1/1000 s\n",
    "        grad_lambda = np.sum(1.0/Z) - T_length\n",
    "        grad_mu = [np.sum(F[j]/Z) - D[j] for j in range(D.shape[0])]\n",
    "        grad_beta = np.sum(A/Z) - 1.0/alpha * B\n",
    "        gradient = np.concatenate(([grad_lambda], grad_mu, [grad_beta]), axis=0)\n",
    "        return -loglikelihood, -gradient\n",
    "\n",
    "    def check_gradient(self, alpha, F, D, data, observation_window, delta=10**-8):\n",
    "        A, B = self._precompute(alpha, data, observation_window)\n",
    "        print 'Checking gradient...'\n",
    "        for i in range(50):\n",
    "            row_idx = np.random.randint(0, len(data))\n",
    "            sequence = data[row_idx]\n",
    "            a2 = A[row_idx]\n",
    "            b = B[row_idx]\n",
    "            f = F[row_idx]\n",
    "            init_lambda = np.random.uniform(10**-6, 1.0)\n",
    "            init_mu = np.random.uniform(10**-6, 1.0, size=D.shape[0])\n",
    "            init_beta = np.random.uniform(10**-6, alpha)\n",
    "            init_params = np.concatenate(([init_lambda], init_mu, [init_beta]), axis=0)\n",
    "            \"\"\"\n",
    "            diff = scipy.optimize.check_grad(lambda p, a, f, d2, d, o, a2, b:\n",
    "                                                 self._negloglikelihood(p, a, f, d2, d, o, a2, b)[0],\n",
    "                                             lambda p, a, f, d2, d, o, a2, b:\n",
    "                                                 self._negloglikelihood(p, a, f, d2, d, o, a2, b)[1],\n",
    "                                             init_params,\n",
    "                                             alpha, f, D, sequence, observation_window, a2, b)\n",
    "            if diff > 10**-3:\n",
    "                print 'Warning: gradient checking failed, diff =', diff\n",
    "            \n",
    "            \"\"\"\n",
    "            #params = np.array([np.random.uniform(), np.random.uniform(), np.random.uniform()])\n",
    "            negloglikelihood, gradient = self._negloglikelihood(init_params, alpha, f, D,\n",
    "                                                                [sequence], observation_window, a2, b)\n",
    "            \n",
    "            param_idx = np.random.randint(len(init_params))\n",
    "            #param_idx = len(init_params) - 1\n",
    "            perturbed_params = np.copy(init_params)\n",
    "            perturbed_params[param_idx] += delta\n",
    "            \n",
    "            #perturbed_A, perturbed_B, perturbed_W = self._compute_A_B([sequence], perturbed_params)\n",
    "            new_negloglikelihood, new_gradient = self._negloglikelihood(perturbed_params, alpha, f, D,\n",
    "                                                                        [sequence], observation_window, a2, b)\n",
    "            \n",
    "            numerical_gradient = (new_negloglikelihood - negloglikelihood)/delta\n",
    "            #if abs((gradient[param_idx] - numerical_gradient)/numerical_gradient)  > 0.001:\n",
    "            if not np.isclose(gradient[param_idx], numerical_gradient):\n",
    "                print 'Gradient checking failed for parameter:', param_idx\n",
    "                print 'Analytical gradient:', gradient[param_idx],\n",
    "                print 'Numerical gradient:', (new_negloglikelihood - negloglikelihood)/delta\n",
    "    \n",
    "    def predict_dist(self, t0, prev_t, niter=1000):\n",
    "        step = 3600.0 - (t0 % 3600)\n",
    "\n",
    "        simulated_iats = np.zeros(shape=(niter,))\n",
    "        for i in range(niter):\n",
    "            t = 0.0\n",
    "            while True:\n",
    "                upper_bound = self.intensity(t0 + t, prev_t, include_t0=True)\n",
    "                s = 1000.0 * np.random.exponential(1.0/upper_bound) # params are in 1/1000s units\n",
    "            \n",
    "                if s >= step: # upper bound no longer valid\n",
    "                    t += step\n",
    "                    step = 3600.0 # one hour in seconds\n",
    "                    continue\n",
    "                \n",
    "                t += s # move ahead in time (may accept or reject)\n",
    "                step -= s\n",
    "                u = np.random.uniform(0.0, 1.0)\n",
    "                lambda_t = self.intensity(t0 + t, prev_t, include_t0=False)\n",
    "                \n",
    "                if u <= lambda_t/upper_bound: # accept\n",
    "                    simulated_iats[i] = t\n",
    "                    break\n",
    "    \n",
    "        return simulated_iats\n",
    "\n",
    "    def intensity(self, t0, prev_t, include_t0=False):\n",
    "        if not include_t0:\n",
    "            prev_t_ = prev_t[prev_t < t0]\n",
    "        else:\n",
    "            prev_t_ = prev_t[prev_t <= t0]\n",
    "        t0_minus_ti = (t0 - prev_t_) / 1000.0\n",
    "        \n",
    "        t0_ = time.gmtime(t0)\n",
    "        hour_of_day, day_of_week, day_of_month = map(int, time.strftime(\"%H %w %d\", t0_).split(' '))\n",
    "        day_of_week = (day_of_week - 1) % 7\n",
    "        \n",
    "        f = np.array([int(hour_of_day == h) for h in range(24)] +\n",
    "                     [int(day_of_week >= 0 and day_of_week <= 3)] +\n",
    "                     [int(day_of_week == 4)] +\n",
    "                     [int(day_of_week >= 5 and day_of_week <= 6)] +\n",
    "                     [int(day_of_month == 1)], dtype=np.int)\n",
    "\n",
    "        intensity = self._lambda\n",
    "        intensity += self._beta * np.sum(np.exp(-self._alpha * t0_minus_ti))\n",
    "        intensity += np.sum(self._mu * f)\n",
    "        return intensity\n",
    "    \n",
    "    def intensity_integral(self, start_t, end_t, data_epochs):\n",
    "        integral_value = 0.0\n",
    "        \n",
    "        integral_value += self._lambda * (end_t - start_t) / 1000.0\n",
    "        \n",
    "        # mu term integral\n",
    "        d = np.zeros(28, dtype=np.float64)\n",
    "        start_timestamp = pd.to_datetime(start_t, unit='s')\n",
    "        end_timestamp = pd.to_datetime(end_t, unit='s')\n",
    "\n",
    "        #print 't_i-1', start_timestamp, 't_i', end_timestamp\n",
    "        \n",
    "        start_ts_roundup = start_timestamp.floor(freq='H')\n",
    "        start_seconds = (start_timestamp - start_ts_roundup).total_seconds()\n",
    "        start_hour = start_timestamp.hour\n",
    "        end_ts_roundwn = end_timestamp.ceil(freq='H')\n",
    "        end_seconds = (end_ts_roundwn - end_timestamp).total_seconds()\n",
    "        end_hour = end_timestamp.hour\n",
    "\n",
    "        #print 't_i-1', start_timestamp, 't_i', end_timestamp\n",
    "        \n",
    "        #hour_periods = pd.date_range(start_timestamp, end_timestamp, freq='s', closed='left')\n",
    "        hour_periods2 = pd.date_range(start_ts_roundup, end_ts_roundwn, freq='H', closed='left')\n",
    "        #print hour_periods\n",
    "        #hour_counts = Counter(hour_periods.hour)\n",
    "        hour_counts2 = Counter(hour_periods2.hour)\n",
    "        hour_counts2[start_hour] -= start_seconds/3600.0\n",
    "        hour_counts2[end_hour] -= end_seconds/3600.0\n",
    "        #print hour_counts2\n",
    "        #print start_timestamp, start_ts_roundup, end_timestamp, end_ts_roundwn\n",
    "        for h in range(24):\n",
    "            #d[h] = hour_counts[h] / 1000.0\n",
    "            d[h] = (hour_counts2[h] * 3600.0) / 1000.0\n",
    "            #print h, hour_counts[h], hour_counts2[h] * 3600.0\n",
    "            #assert abs(d[h] - (hour_counts2[h] * 3600.0) / 1000.0) < 10**-9\n",
    "        \n",
    "        start_ts_roundup = start_timestamp.floor(freq='D')\n",
    "        start_seconds = (start_timestamp - start_ts_roundup).total_seconds()\n",
    "        start_day = start_timestamp.weekday()\n",
    "        end_ts_roundwn = end_timestamp.ceil(freq='D')\n",
    "        end_seconds = (end_ts_roundwn - end_timestamp).total_seconds()\n",
    "        end_day = end_timestamp.weekday()\n",
    "        \n",
    "        #print start_timestamp, start_ts_roundup, start_seconds/3600.0, end_timestamp, end_ts_roundwn, end_seconds/3600.0\n",
    "        #day_counts = Counter(hour_periods.weekday)\n",
    "        day_periods = pd.date_range(start_ts_roundup, end_ts_roundwn, freq='D', closed='left')\n",
    "        day_counts2 = Counter(day_periods.weekday)\n",
    "        day_counts2[start_day] -= start_seconds/86400.0\n",
    "        day_counts2[end_day] -= end_seconds/86400.0\n",
    "        #for d_i in range(7):\n",
    "        #    assert abs(day_counts[d_i]/86400.0 - day_counts2[d_i]) < 10**-9\n",
    "        d[24] = (day_counts2[0] + day_counts2[1] + day_counts2[2] + day_counts2[3]) * 86400.0 / 1000.0\n",
    "        d[25] = day_counts2[4] * 86400.0/ 1000.0\n",
    "        d[26] = (day_counts2[5] + day_counts2[6]) * 86400.0 / 1000.0\n",
    "        \n",
    "        num_month_starts = np.sum(day_periods.is_month_start)\n",
    "        #print num_month_starts\n",
    "        if start_timestamp.is_month_start:\n",
    "            num_month_starts -= start_seconds/86400.0\n",
    "        if end_timestamp.is_month_start:\n",
    "            num_month_starts -= end_seconds/86400.0\n",
    "        #print start_timestamp, end_timestamp, np.sum(hour_periods.is_month_start), num_month_starts*86400.0\n",
    "        #assert abs(np.sum(hour_periods.is_month_start) - num_month_starts*86400.0) < 10**-9\n",
    "        #d[27] = np.sum(hour_periods.is_month_start) / 1000.0\n",
    "        d[27] = num_month_starts * 86400.0 / 1000.0\n",
    "        d = np.array(d)\n",
    "        #print d\n",
    "        \n",
    "        mu_term = np.sum(self._mu * d)\n",
    "        integral_value += mu_term\n",
    "                \n",
    "        epochs_n_ = data_epochs[data_epochs <= start_t]\n",
    "        term1 = np.sum(np.exp(-self._alpha * (start_t - epochs_n_) / 1000.0) -\n",
    "                       np.exp(-self._alpha * (end_t - epochs_n_) / 1000.0))\n",
    "\n",
    "        #epochs_n_ = data_epochs[(data_epochs >= start_t) & (data_epochs < end_t)]\n",
    "        #term2 = np.sum(1 - np.exp(-self._alpha * (end_t - epochs_n_) / 1000.0))\n",
    "\n",
    "        integral_value += self._beta/self._alpha * term1\n",
    "        \n",
    "        return integral_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
